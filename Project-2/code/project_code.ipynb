{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST on Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pylab import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "filename = 'D:\\sem1\\intro to ml\\project3\\mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "#print(len(training_data))\n",
    "#print(training_data[1])\n",
    "x_train= training_data[0]\n",
    "y_train = training_data[1]\n",
    "x=training_data[0]\n",
    "y=training_data[1]\n",
    "x_test = test_data[0]\n",
    "y_test = test_data[1]\n",
    "val_x = validation_data[0]\n",
    "val_y = validation_data[1]\n",
    "x_train,y_train = training_data\n",
    "x_test,y_test = test_data\n",
    "print(len(y_test ))\n",
    "#print(tx)\n",
    "#print(ty)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load USPS on Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19999\n"
     ]
    }
   ],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = r'D:\\sem1\\intro to ml\\project3\\USPSdata\\USPSdata\\Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)\n",
    "us_testx = USPSMat[0:2000]\n",
    "us_testy = USPSMat[0:2000]\n",
    "testx= USPSMat\n",
    "testy = USPSTar\n",
    "testxnn= np.asarray(USPSMat)\n",
    "num_classes = 10\n",
    "USPSTarnn = keras.utils.to_categorical(USPSTar, num_classes)\n",
    "testynn = USPSTarnn\n",
    "print(len(testx ) )\n",
    "#print(len(testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_log_reg(train_features, target, steps):\n",
    "    weight_matrix = []\n",
    "    a_mat = np.zeros((10))\n",
    "    y = np.zeros((10))\n",
    "\n",
    "    for i in range(10):\n",
    "        dim1 = np.zeros(train_features.shape[1])\n",
    "        weight_matrix.append(dim1)\n",
    "        \n",
    "    for step in range(steps):\n",
    "        x_row = train_features[step]\n",
    "        tar_row = target[step]\n",
    "        total = 0\n",
    "        for i in range(10):\n",
    "            bias_value = 0\n",
    "            a_mat[i] = np.dot(weight_matrix[i].T, x_row) + bias_value\n",
    "            total = total + np.exp(a_mat[i])\n",
    "        for i in range(10):\n",
    "            y[i] = (np.exp(a_mat[i])) / (total)\n",
    "        for i in range(10):\n",
    "            computed_diff = (y[i] - tar_row[i]) * x_row\n",
    "            weight_matrix[i] = weight_matrix[i] - (0.01 * computed_diff)\n",
    "\n",
    "    return weight_matrix\n",
    "def oneHotIt(Y):\n",
    "    m = Y.shape[0]\n",
    "    targets = np.array(Y)\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    #OH = np.zeros((m,10))\n",
    "    #OH[(np.arange(m),target)] =1\n",
    "    return OHX\n",
    "\n",
    "def prediction(train_features, target, steps, wx):\n",
    "    weight_matrix = wx\n",
    "    a_mat = np.zeros((10))\n",
    "    yy = np.zeros(steps)\n",
    "    for i in range(10):\n",
    "        dim1 = np.zeros(train_features.shape[1])\n",
    "        weight_matrix.append(dim1)\n",
    "        \n",
    "    for step in range(steps):\n",
    "        x_row = train_features[step]\n",
    "        tar_row = target[step]\n",
    "        total = 0\n",
    "        for i in range(10):\n",
    "            bias_value = 0\n",
    "            a_mat[i] = np.dot(weight_matrix[i].T, x_row) + bias_value\n",
    "            total = total + np.exp(a_mat[i])\n",
    "        y = np.zeros((10))\n",
    "\n",
    "        for i in range(10):\n",
    "            y[i] = (np.exp(a_mat[i])) / (total)\n",
    "        #print(\"Ypreds\")\n",
    "        #print(np.argmax(y))\n",
    "        #print(tar_row)\n",
    "        \n",
    "        yy[step] = (np.argmax(y)).astype(int)\n",
    "    return yy\n",
    "\n",
    "def predictionUSPS(train_features, target, steps, wx):\n",
    "    weight_matrix = wx\n",
    "    a_mat = np.zeros((10))\n",
    "    yy = np.zeros(steps)\n",
    "    for i in range(10):\n",
    "        dim1 = np.zeros(len(train_features))\n",
    "        weight_matrix.append(dim1)\n",
    "        \n",
    "    for step in range(steps):\n",
    "        x_row = train_features[step]\n",
    "        tar_row = target[step]\n",
    "        total = 0\n",
    "        for i in range(10):\n",
    "            bias_value = 0\n",
    "            a_mat[i] = np.dot(weight_matrix[i].T, x_row) + bias_value\n",
    "            total = total + np.exp(a_mat[i])\n",
    "        y = np.zeros((10))\n",
    "\n",
    "        for i in range(10):\n",
    "            y[i] = (np.exp(a_mat[i])) / (total)\n",
    "        #print(\"Ypreds\")\n",
    "        #print(np.argmax(y))\n",
    "        #print(tar_row)\n",
    "        \n",
    "        yy[step] = (np.argmax(y)).astype(int)\n",
    "    return yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Accuracy : 0.907\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-87211b1494b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mout_mnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"confusion matrix \"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "#w = np.zeros([x.shape[1],len(np.unique(y))])\n",
    "train_features = x_train\n",
    "target = oneHotIt(y_train)\n",
    "steps = 10000\n",
    "steps_train = 50000\n",
    "def getacc(ma,tar):\n",
    "    acc = 0\n",
    "    for i in range(0,len(ma)):\n",
    "        if(int(ma[i]) == tar[i]):\n",
    "            acc = acc + 1\n",
    "    return (acc / 10000)       \n",
    "\n",
    "wx = perform_log_reg(train_features, target, steps_train)\n",
    "ma = prediction(x_test,y_test,steps, wx)\n",
    "#ma_usps = prediction(testx,testy,steps,wx)\n",
    "xx_mnist = []\n",
    "tar = y_test\n",
    "#tar_usps = testy\n",
    "#print(tar)\n",
    "for ele in ma:\n",
    "    xx_mnist.append(int(ele))\n",
    "#print(xx)    \n",
    "print(\"MNIST Accuracy :\" , getacc(xx_mnist,tar))\n",
    "#print(\"USPS Accuracy :\", getacc(ms_usps,tar_usps))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix  [[ 958    0    1    5    0    2    8    2    4    0]\n",
      " [   0 1113    0    5    0    2    3    2   10    0]\n",
      " [  12   11  881   35    6    4   13   21   42    7]\n",
      " [   1    0   11  919    0   45    0   13   14    7]\n",
      " [   2    3    3    2  881    0   13    6   12   60]\n",
      " [  10    5    1   40    4  780    8    8   31    5]\n",
      " [  14    3    3    2    7   36  884    3    6    0]\n",
      " [   2   11   19    9    5    0    0  956    5   21]\n",
      " [   7   10    3   44    7   66    7   17  801   12]\n",
      " [   8    8    0   11   17   19    1   45    3  897]]\n"
     ]
    }
   ],
   "source": [
    "out_mnist = confusion_matrix(y_test, ma)\n",
    "print(\"confusion matrix :\" , out_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w = np.zeros([x.shape[1],len(np.unique(y))])\n",
    "train_features = x_train\n",
    "target = oneHotIt(y_train)\n",
    "steps = 19999\n",
    "def getacc(ma,tar):\n",
    "    acc = 0\n",
    "    for i in range(0,len(ma)):\n",
    "        if(int(ma[i]) == tar[i]):\n",
    "            acc = acc + 1\n",
    "    return (acc / 19999)\n",
    "wx = perform_log_reg(train_features, target, steps)\n",
    "#ma = prediction(x_test,y_test,steps, wx)\n",
    "ma_usps = predictionUSPS(USPSMat,testy,steps,wx)\n",
    "xx_usps = []\n",
    "tar_usps = testy\n",
    "#print(tar)\n",
    "for ele in ma_usps:\n",
    "    xx_usps.append(ele)\n",
    "print(\"USPS Accuracy :\", getacc(ma_usps,tar_usps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix  [[ 477    5  239   69  217  287   90   76  225  315]\n",
      " [ 137  337  106  310  213  103   38  442  303   11]\n",
      " [ 134   28 1118  155   46  160   84   95  154   25]\n",
      " [  76    4  107 1155   14  412   11   71  110   40]\n",
      " [  47   73   30   47  936  143   53  161  348  162]\n",
      " [ 133   21  149  144   29 1260  103   63   79   19]\n",
      " [ 190   15  376  109   72  428  684   17   73   36]\n",
      " [ 163  218  170  423   73  126   20  386  371   50]\n",
      " [ 191   35   98  172   93  758   88   38  460   67]\n",
      " [  33  171  105  431  136  106   14  430  406  168]]\n"
     ]
    }
   ],
   "source": [
    "out_usps = confusion_matrix(testy, ma_usps)\n",
    "print(\"confusion matrix \" , out_usps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USPS:  0.3168658432891842\n"
     ]
    }
   ],
   "source": [
    "num_classes=10\n",
    "image_vector_size=28*28\n",
    "x_trainn = x_train.reshape(x_train.shape[0], image_vector_size)\n",
    "x_testn = x_test.reshape(x_test.shape[0], image_vector_size)\n",
    "val_x = val_x.reshape(val_x.shape[0], image_vector_size)\n",
    "val_y = keras.utils.to_categorical(val_y, num_classes)\n",
    "y_trainn = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_testn = keras.utils.to_categorical(y_test, num_classes)\n",
    "image_size = 784\n",
    "USPSTa = USPSTar\n",
    "USPSTar = keras.utils.to_categorical(USPSTar, num_classes)\n",
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='sigmoid', input_shape=(image_size,)))\n",
    "model.add(Dense(units=50, activation='relu',input_shape=(image_size,)))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(units=10, activation='softmax',input_shape=(image_size,)))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history = model.fit(x_trainn, y_trainn, batch_size=6000, epochs=600,verbose=False,validation_split=.1)\n",
    "loss1,accuracy1 = model.evaluate(testxnn,testynn,verbose=False)\n",
    "#print(\"MNIST: \"  ,loss)\n",
    "print(\"USPS: \" ,accuracy1)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-346ea32d0d3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpnn_usps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestxnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpnn_us\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestxnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnn_out_usps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtar_nn_us\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpnn_usps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Neural Network confusion matrix USPS: \"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnn_out_usps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and multiclass targets"
     ]
    }
   ],
   "source": [
    "tar_nn_us = np.asarray(USPSTar)\n",
    "pnn_usps = model.predict_classes(testxnn)\n",
    "pnn_us = model.predict(testxnn)\n",
    "nn_out_usps = confusion_matrix(tar_nn_us, pnn_usps)\n",
    "print(\"Neural Network confusion matrix USPS: \" , nn_out_usps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST:  0.9615\n",
      "Neural network confusion matrix MNIST:  [[ 961    0    3    2    1    2    5    2    2    2]\n",
      " [   0 1118    6    1    0    1    2    2    5    0]\n",
      " [   3    1  988    8    4    0    6    9   12    1]\n",
      " [   2    3    8  963    1    9    0   12    9    3]\n",
      " [   1    1    2    1  947    0    9    5    2   14]\n",
      " [   4    1    1   10    5  849    8    3    5    6]\n",
      " [   7    2    3    1    5    4  931    0    5    0]\n",
      " [   1    3   11    7    2    1    0  993    1    9]\n",
      " [   5    1    8   15    4   10    6    6  915    4]\n",
      " [   4    2    0   12   20    6    1    6    8  950]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss,accuracy = model.evaluate(x_testn, y_testn, verbose=False)\n",
    "#loss,accuracy_val = model.evaluate(val_x, val_y, verbose=False)\n",
    "pnn_mnist = model.predict_classes(x_testn)\n",
    "#pnn_mnist = pnn_mnist_x\n",
    "print(\"MNIST: \" ,accuracy)\n",
    "#print(\"MNIST Val: \" ,accuracy_val)\n",
    "nn_out_usps = confusion_matrix(y_test, pnn_mnist)\n",
    "print(\"Neural network confusion matrix MNIST: \" , nn_out_usps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "X_train, Y_train = training_data\n",
    "X_test, Y_test = test_data\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "#testy = USPSTar\n",
    "###########################\n",
    "testxcnn= np.asarray(USPSMat)\n",
    "num_classes = 10\n",
    "#USPSTarcnn = keras.utils.to_categorical(USPSTar, num_classes)\n",
    "USPSTarcnn = USPSTar\n",
    "testycnn = USPSTarcnn\n",
    "\n",
    "testxcnn = testxcnn.reshape(testxcnn.shape[0], 1, 28, 28).astype('float32')\n",
    "#testycnn = X_test.reshape(testycnn.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "testxcnn = testxcnn / 255\n",
    "# one hot encode outputs\n",
    "testycnn = np_utils.to_categorical(testycnn)\n",
    "####################\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "num_classes = Y_test.shape[1]\n",
    "\n",
    "def cnn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))# Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 2s - loss: 1.4219 - acc: 0.5376 - val_loss: 0.5475 - val_acc: 0.8370\n",
      "Epoch 2/20\n",
      " - 2s - loss: 0.5781 - acc: 0.8183 - val_loss: 0.4054 - val_acc: 0.8843\n",
      "Epoch 3/20\n",
      " - 2s - loss: 0.4588 - acc: 0.8596 - val_loss: 0.3173 - val_acc: 0.9078\n",
      "Epoch 4/20\n",
      " - 2s - loss: 0.3693 - acc: 0.8879 - val_loss: 0.2515 - val_acc: 0.9280\n",
      "Epoch 5/20\n",
      " - 2s - loss: 0.3039 - acc: 0.9077 - val_loss: 0.2056 - val_acc: 0.9376\n",
      "Epoch 6/20\n",
      " - 2s - loss: 0.2575 - acc: 0.9205 - val_loss: 0.1683 - val_acc: 0.9529\n",
      "Epoch 7/20\n",
      " - 2s - loss: 0.2235 - acc: 0.9320 - val_loss: 0.1461 - val_acc: 0.9569\n",
      "Epoch 8/20\n",
      " - 2s - loss: 0.2000 - acc: 0.9381 - val_loss: 0.1262 - val_acc: 0.9623\n",
      "Epoch 9/20\n",
      " - 2s - loss: 0.1785 - acc: 0.9441 - val_loss: 0.1123 - val_acc: 0.9671\n",
      "Epoch 10/20\n",
      " - 2s - loss: 0.1592 - acc: 0.9500 - val_loss: 0.0979 - val_acc: 0.9700\n",
      "Epoch 11/20\n",
      " - 2s - loss: 0.1462 - acc: 0.9546 - val_loss: 0.0926 - val_acc: 0.9717\n",
      "Epoch 12/20\n",
      " - 2s - loss: 0.1333 - acc: 0.9586 - val_loss: 0.0845 - val_acc: 0.9731\n",
      "Epoch 13/20\n",
      " - 2s - loss: 0.1232 - acc: 0.9620 - val_loss: 0.0753 - val_acc: 0.9750\n",
      "Epoch 14/20\n",
      " - 2s - loss: 0.1152 - acc: 0.9638 - val_loss: 0.0723 - val_acc: 0.9777\n",
      "Epoch 15/20\n",
      " - 2s - loss: 0.1059 - acc: 0.9675 - val_loss: 0.0667 - val_acc: 0.9795\n",
      "Epoch 16/20\n",
      " - 2s - loss: 0.0993 - acc: 0.9694 - val_loss: 0.0624 - val_acc: 0.9796\n",
      "Epoch 17/20\n",
      " - 2s - loss: 0.0935 - acc: 0.9710 - val_loss: 0.0583 - val_acc: 0.9808\n",
      "Epoch 18/20\n",
      " - 2s - loss: 0.0879 - acc: 0.9727 - val_loss: 0.0576 - val_acc: 0.9821\n",
      "Epoch 19/20\n",
      " - 2s - loss: 0.0833 - acc: 0.9735 - val_loss: 0.0532 - val_acc: 0.9835\n",
      "Epoch 20/20\n",
      " - 2s - loss: 0.0804 - acc: 0.9745 - val_loss: 0.0520 - val_acc: 0.9837\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_15 to have 2 dimensions, but got array with shape (19999, 10, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-b81544f9f607>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mscores_mnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpcnn_mnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mscore_usps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestxcnn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestycnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mpcnn_usps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestxcnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1101\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    125\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    128\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_15 to have 2 dimensions, but got array with shape (19999, 10, 2)"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = cnn_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores_mnist = model.evaluate(X_test, Y_test, verbose=0)\n",
    "pcnn_mnist = model.predict(X_test)\n",
    "score_usps = model.evaluate(testxcnn,testycnn, verbose=0)\n",
    "pcnn_usps = model.predict(testxcnn)\n",
    "#print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_mnist[len(scores_mnist)-3])\n",
    "print(score_usps[len(score_usps)-3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3064653232661633\n",
      "RF confusion matrix USPS:  [[643  41 307  69 346 176 132 112  12 162]\n",
      " [ 89 497 202 128  48  78  50 871  29   8]\n",
      " [220  98 910 150  87 200  70 230  21  13]\n",
      " [125  53 191 994  88 350  21 126  30  22]\n",
      " [ 42 194 183 101 846 153  56 321  39  65]\n",
      " [210  92 236 277  74 891  70 109  14  27]\n",
      " [351  85 374 103 155 284 511  95  12  30]\n",
      " [111 385 323 270  48 186  37 603  22  15]\n",
      " [145 123 281 268 160 653  96 100 122  52]\n",
      " [ 71 327 300 317 218 133  30 407  85 112]]\n"
     ]
    }
   ],
   "source": [
    "# SVM & RandomForest\n",
    "n_train = 50000\n",
    "n_test = 10000\n",
    "indices = arange(len(x_train))\n",
    "train_idx = arange(0,n_train)\n",
    "test_idx = arange(n_train+1,n_train+n_test)\n",
    "#X_train, y_train = mnist.data[train_idx], mnist.target[train_idx]\n",
    "#X_test, y_test = mnist.data[test_idx], mnist.target[test_idx]\n",
    "classifier2 = RandomForestClassifier(n_estimators=10)\n",
    "classifier2.fit(x_train, y_train)\n",
    "rf_preds_mnist = classifier2.predict(x_test)\n",
    "rf_preds_usps = classifier2.predict(testx)\n",
    "#print((preds == y_train).mean())\n",
    "\n",
    "acc_rf = accuracy_score(testy, rf_preds_usps)\n",
    "print(acc_rf)\n",
    "# SVM\n",
    "rf_out_usps = confusion_matrix(testy, rf_preds_usps)\n",
    "print(\"RF confusion matrix USPS: \" , rf_out_usps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix MNIST:  [[ 965    0    0    1    0    2    7    2    3    0]\n",
      " [   0 1117    3    4    1    2    5    0    3    0]\n",
      " [  11    3  984    7    4    2    3   10    7    1]\n",
      " [   5    3   17  930    0   25    1   11   14    4]\n",
      " [   3    0    7    3  934    2    9    1    3   20]\n",
      " [  10    3    4   30   10  814    8    2    6    5]\n",
      " [  12    3    4    2    6    9  918    1    3    0]\n",
      " [   1    8   19    8   10    0    0  959    3   20]\n",
      " [   6    2   14   14    7   13    7    8  894    9]\n",
      " [   8    5    2   16   30    6    2    8   15  917]]\n",
      "0.9432\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "acc_rf_mnist =  accuracy_score(y_test, rf_preds_mnist)\n",
    "rf_out_mnist = confusion_matrix(y_test, rf_preds_mnist)\n",
    "print(\"RF confusion matrix MNIST: \" , rf_out_mnist)\n",
    "print(acc_rf_mnist)\n",
    "print(len(rf_preds_mnist) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jayanth\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9435\n",
      "0.38541927096354817\n",
      "SVM done\n"
     ]
    }
   ],
   "source": [
    "classifier1 = SVC(kernel='rbf')\n",
    "classifier1.fit(x_train, y_train)\n",
    "#print((testx.shape()))\n",
    "sv_preds1_mnist = classifier1.predict(x_test)\n",
    "sv_preds1_usps = classifier1.predict(testx)\n",
    "print((sv_preds1_mnist == y_test).mean())\n",
    "print((sv_preds1_usps == testy).mean())\n",
    "\n",
    "print(\"SVM done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix MNIST:  [[ 967    0    1    0    0    5    4    1    2    0]\n",
      " [   0 1120    2    3    0    1    3    1    5    0]\n",
      " [   9    1  962    7   10    1   13   11   16    2]\n",
      " [   1    1   14  950    1   17    1   10   11    4]\n",
      " [   1    1    7    0  937    0    7    2    2   25]\n",
      " [   7    4    5   33    7  808   11    2   10    5]\n",
      " [  10    3    4    1    5   10  924    0    1    0]\n",
      " [   2   13   22    5    7    1    0  954    4   20]\n",
      " [   4    6    6   14    8   24   10    8  891    3]\n",
      " [  10    6    0   12   33    5    1   14    6  922]]\n"
     ]
    }
   ],
   "source": [
    "sv_out_mnist = confusion_matrix(y_test, sv_preds1_mnist)\n",
    "print(\"RF confusion matrix MNIST: \" , sv_out_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF confusion matrix USPS:  [[ 573    2  428   19  285  248   73   44    6  322]\n",
      " [ 110  429  285  137  273  180   46  501   22   17]\n",
      " [ 128   18 1402   59   39  198   61   57   23   14]\n",
      " [  76    3  186 1123   11  483    5   70   27   16]\n",
      " [  18   67   91   14 1167  267   22  194   69   91]\n",
      " [ 108   17  257  102   25 1367   60   43   15    6]\n",
      " [ 197    7  489   24   98  394  748   13    7   23]\n",
      " [  50  225  457  265   57  416   15  452   41   22]\n",
      " [  73   25  209  193   87 1006   95   41  244   27]\n",
      " [  26  166  228  278  213  165    8  499  214  203]]\n"
     ]
    }
   ],
   "source": [
    "sv_out_usps = confusion_matrix(testy, sv_preds1_usps)\n",
    "print(\"RF confusion matrix USPS: \" , sv_out_usps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7. 4. 2. ... 4. 6. 4.]\n"
     ]
    }
   ],
   "source": [
    "#### Ensemble\n",
    "from scipy import stats\n",
    "final_pred = np.array([])\n",
    "for i in range(0,len(x_test)):\n",
    "    final_pred = np.append(final_pred, stats.mode([xx_mnist[i],rf_preds_mnist[i],pnn_mnist[i],sv_preds1_mnist[i]]) )\n",
    "print(final_pred)\n",
    "fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final acc mnist :  0.0978\n"
     ]
    }
   ],
   "source": [
    "finalacc = 0\n",
    "\n",
    "for i in range(0,len(x_test)):\n",
    "    if(int( int(final_pred[i]) ) == int(y_test[i]) ):\n",
    "        finalacc = finalacc + 1\n",
    "print(\"final acc mnist : \" ,finalacc / 10000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
